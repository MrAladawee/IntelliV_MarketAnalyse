import os
import time
import json
import csv
import re
from datetime import datetime, date, timedelta

import warnings
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup
import pandas as pd
import numpy as np
import torch
import joblib
from transformers import AutoTokenizer, AutoModel, logging as hf_logging
import torch.nn as nn
from filelock import FileLock

# ================= Telegram config =================
from config import TELEGRAM_TOKEN, CHAT_IDS
hf_logging.set_verbosity_error()
warnings.filterwarnings("ignore", message="X does not have valid feature names")

# ================= Parser settings =================
TICKERS        = ['ABIO', 'AFKS', 'AFLT', 'AKRN', 'ALRS', 'APTK', 'AQUA', 'ASTR', 'BELU', 'BSPB', 
                  'CBOM', 'CHMF', 'DATA', 'DELI', 'DIAS', 'ELFV', 'ENPG', 'EUTR', 'FEES', 'FESH', 
                  'FLOT', 'GAZP', 'GEMC', 'GMKN', 'HEAD', 'HNFG', 'HYDR', 'IRAO', 'IVAT', 'KMAZ', 
                  'LEAS', 'LENT', 'LKOH', 'LSRG', 'MAGN', 'MBNK', 'MDMG', 'MOEX', 'MRKC', 'MRKP', 
                  'MRKS', 'MRKU', 'MRKV', 'MRKZ', 'MSNG', 'MSRS', 'MTLR', 'MTSS', 'MVID', 
                  'NLMK', 'NVTK', 'OGKB', 'OZPH', 'PHOR', 'PIKK', 'PLZL', 'POSI', 'PRMD', 'RASP', 
                  'RENI', 'RNFT', 'ROSN', 'RTKM', 'RUAL', 'SBER', 'SELG', 'SFIN', 
                  'SGZH', 'SMLT', 'SNGS', 'SOFL', 'SVAV', 'SVCB', 'TATN', 'TGKA', 
                  'TGKN', 'TRMK', 'TRNFP', 'T', 'UGLD', 'UPRO', 'VKCO', 'VSEH', 'VSMO', 'VTBR', 'WUSH', 
                  'X5', 'YDEX']

SPECIAL_TICKERS = {
    "SBER": ["SBERP"],
    "RTKM": ["RTKMP"],
    "TATN": ["TATNP"],
    "SNGS": ["SNGSP"],
    "MTLR": ["MTLRP"]
}

BASE_URL       = "https://smart-lab.ru/forum/news/{ticker}/page1/"
LAST_SEEN_FILE = "last_seen.json"
CHECK_INTERVAL = 68       # —Å–µ–∫—É–Ω–¥—ã
DAILY_DATA_DIR = "data_ticker"
# ================================================

# ================ HTTP session with retries ================
session = requests.Session()
retries = Retry(
    total=5,
    backoff_factor=1,
    status_forcelist=[500, 502, 503, 504],
    allowed_methods=["GET"]
)
adapter = HTTPAdapter(max_retries=retries)
session.mount("https://", adapter)
session.mount("http://", adapter)

def safe_get(url, **kwargs):
    """–û–±—ë—Ä—Ç–∫–∞ session.get —Å –ø–æ–≤—Ç–æ—Ä–æ–º –Ω–∞ SSLError, –ª–æ–≥–æ–º –∏ –∞–Ω—Ç–∏-–∫—ç—à –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º."""
    # –î–æ–±–∞–≤–ª—è–µ–º –∞–Ω—Ç–∏-–∫—ç—à –ø–∞—Ä–∞–º–µ—Ç—Ä
    params = kwargs.get("params", {})
    params["_"] = int(time.time())  # —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è –æ–±—Ö–æ–¥–∞ –∫—ç—à–∞
    kwargs["params"] = params

    try:
        return session.get(url, timeout=10, **kwargs)
    except requests.exceptions.SSLError:
        time.sleep(5)
        return session.get(url, timeout=10, **kwargs)
    except Exception as e:
        print(f"[WARN] HTTP error accessing {url}: {e}", flush=True)
        return None

# ================ Model & scalers ================
SCALER_X_PATH = os.path.join("..", "scaler_X.pkl")
SCALER_Y_PATH = os.path.join("..", "scaler_y.pkl")
MODEL_PATH    = os.path.join("..", "clean_news_30d_LSTM.pth")

MODEL_NAME    = "DeepPavlov/rubert-base-cased"
#MODEL_NAME = "./models/rubert"

DEVICE        = torch.device("cuda" if torch.cuda.is_available() else "cpu")

scaler_X   = joblib.load(SCALER_X_PATH)
scaler_y   = joblib.load(SCALER_Y_PATH)

tokenizer  = AutoTokenizer.from_pretrained(MODEL_NAME)
bert_model = AutoModel.from_pretrained(MODEL_NAME)

#tokenizer  = AutoTokenizer.from_pretrained(MODEL_NAME, local_files_only=True)
#bert_model = AutoModel.from_pretrained(MODEL_NAME, local_files_only=True)

class MultiModalModel(nn.Module):
    def __init__(self, bert_model, seq_len, lstm_hidden_dim=64):
        super().__init__()
        self.bert = bert_model
        self.text_dropout = nn.Dropout(0.2)
        self.norm_text   = nn.LayerNorm(bert_model.config.hidden_size)
        self.text_branch = nn.Sequential(
            nn.Linear(bert_model.config.hidden_size, 128),
            nn.GELU()
        )
        self.lstm = nn.LSTM(
            input_size=1, hidden_size=lstm_hidden_dim,
            num_layers=1, batch_first=True, bidirectional=True
        )
        self.norm_num        = nn.LayerNorm(lstm_hidden_dim * 2)
        self.num_to_text_dim = nn.Linear(lstm_hidden_dim * 2, 128)
        self.film_gen = nn.Sequential(
            nn.Linear(128, 256), nn.GELU(), nn.Linear(256, 256)
        )
        self.head = nn.Sequential(
            nn.Linear(256, 128), nn.GELU(), nn.Dropout(0.2),
            nn.Linear(128, 64), nn.GELU(), nn.Dropout(0.2),
            nn.Linear(64, 1)
        )

    def forward(self, input_ids, attention_mask, X_num):
        bert_out = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        cls_emb  = bert_out.last_hidden_state[:, 0, :]
        cls_emb  = self.text_dropout(self.norm_text(cls_emb))
        cls_feat = self.text_branch(cls_emb)

        X_seq        = X_num.unsqueeze(-1)
        _, (h_n, _) = self.lstm(X_seq)
        num_feat     = torch.cat([h_n[0], h_n[1]], dim=1)
        num_feat     = self.norm_num(num_feat)
        num_proj     = self.num_to_text_dim(num_feat)

        film      = self.film_gen(cls_feat)
        gamma, beta = film.chunk(2, dim=1)
        num_mod   = num_proj * (1 + gamma) + beta

        fused = torch.cat([cls_feat, num_mod], dim=1)
        return self.head(fused).squeeze(-1)

model = MultiModalModel(bert_model=bert_model, seq_len=30, lstm_hidden_dim=64)
model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE, weights_only=True))
model.to(DEVICE)
model.eval()
# ================================================

# ============= Price history utils =============
_history_cache = {}
def load_daily_history(ticker):
    if ticker in _history_cache:
        return _history_cache[ticker]
    path = os.path.join(DAILY_DATA_DIR, f"{ticker}_data.csv")
    if not os.path.exists(path):
        df = None
    else:
        df = pd.read_csv(path, parse_dates=["TRADEDATE"])
        df["TRADEDATE"] = df["TRADEDATE"].dt.date
        df = df.sort_values("TRADEDATE").drop_duplicates("TRADEDATE", keep="last")
    _history_cache[ticker] = df
    return df

def get_news_price(ticker, news_dt):
    """
    –ë–µ—Ä—ë–º –º–∏–Ω—É—Ç–Ω—ã–µ —Å–≤–µ—á–∏, –Ω–∞—á–∏–Ω–∞—è –∑–∞ –ø–æ–ª—á–∞—Å–∞ –¥–æ —á–∞—Å–∞ news_dt,
    –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ü–µ–Ω—É, –±–ª–∏–∂–∞–π—à—É—é –∫–æ –≤—Ä–µ–º–µ–Ω–∏ news_dt.
    """
    date_str = news_dt.date().isoformat()
    hour_mark = news_dt.replace(minute=0, second=0, microsecond=0)
    start_dt  = hour_mark - timedelta(minutes=30)

    resp = safe_get(
        f"https://iss.moex.com/iss/engines/stock/markets/shares/securities/{ticker}/candles.json",
        params={
            "from":     start_dt.isoformat(),
            "till":     f"{date_str}T23:59:59",
            "interval": 1,
            "iss.meta": "off"
        }
    )
    if not resp or resp.status_code != 200:
        return None
    data = resp.json().get("candles", {}).get("data", [])
    if not data:
        return None
    best = min(data, key=lambda c: abs(news_dt.timestamp() - datetime.fromisoformat(c[6]).timestamp()))
    return float(best[1])


def get_past_prices(hist, news_date):

    out = {}

    if hist is None:
        return out
    
    dates = hist["TRADEDATE"].tolist()
    idxs  = [i for i,d in enumerate(dates) if d < news_date]

    if not idxs:
        return out
    
    pos = max(idxs)

    for i in range(30):
        j = pos - i
        if j < 0:
            break
        out[f"T-{i+1}"] = float(hist.loc[j, "CLOSE"])

    return out
# ================================================

# –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –ø–∞–ø–∫–∞ –¥–ª—è –≤—ã—Ö–æ–¥–Ω—ã—Ö CSV —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
PROD_DATA_DIR = "prod_data"
os.makedirs(PROD_DATA_DIR, exist_ok=True)

# ================ CSV helpers ==================
def load_last_seen():
    return json.load(open(LAST_SEEN_FILE)) if os.path.exists(LAST_SEEN_FILE) else {}

def save_last_seen(d):
    with open(LAST_SEEN_FILE, "w", encoding="utf-8") as f:
        json.dump(d, f, ensure_ascii=False, indent=2)

def load_rows(ticker):
    """
    –ß–∏—Ç–∞–µ—Ç —Å—Ç—Ä–æ–∫–∏ –∏–∑ prod_data/{ticker}_data.csv
    """
    fname = os.path.join(PROD_DATA_DIR, f"{ticker}_data.csv")
    if not os.path.exists(fname):
        return []
    with open(fname, newline="", encoding="utf-8") as f:
        return list(csv.DictReader(f))

def save_rows(ticker, rows):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–ø–∏—Å–æ–∫ dicts –≤ prod_data/{ticker}_data.csv
    —Ç–µ–ø–µ—Ä—å —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º —Å—Ç–æ–ª–±—Ü–æ–º pct_change
    """
    fname  = os.path.join(PROD_DATA_DIR, f"{ticker}_data.csv")
    fields = [
        "ticker","date","time","news","news_url",
        "T"
    ] + [f"T-{i}" for i in range(1,31)] + [
        "T+1","pct_change"
    ]

    lockfile = fname + ".lock"  # —Ñ–∞–π–ª –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
    lock = FileLock(lockfile)
    with lock:
        with open(fname, "w", newline="", encoding="utf-8") as f:
            w = csv.DictWriter(f, fieldnames=fields)
            w.writeheader()
            w.writerows(rows)
            
# ================================================

# =============== Prediction ====================
def predict_next_price(row):
    price_cols = [f"T-{i}" for i in range(30,0,-1)] + ["T"]
    # –ü—Ä–∏–≤–æ–¥–∏–º –≤—Å–µ –∫ float. –ï—Å–ª–∏ —Ö–æ—Ç—å –æ–¥–∏–Ω –Ω–µ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç—Å—è ‚Äî –≤—ã—Ö–æ–¥–∏–º.
    try:
        prices = [float(row[c]) for c in price_cols]
    except (TypeError, ValueError, KeyError):
        return None

    # –Ø–≤–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –ª–æ–≥-–¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–µ–π T_i / T_{i-1}
    log_rets = []
    for i in range(1, len(prices)):
        prev = prices[i - 1]
        curr = prices[i]
        log_rets.append(np.log(curr / prev))
    log_rets = np.array(log_rets, dtype=float)  # shape (30,)

    X_n = scaler_X.transform(log_rets.reshape(1,-1))
    X_t = torch.tensor(X_n, dtype=torch.float32, device=DEVICE)

    text = f"[TICKER] {row['ticker']} [SEP] {row.get('news','')}"

    enc  = tokenizer(
        text,
        truncation=True,
        padding="max_length",
        max_length=128,
        return_tensors="pt"
    )
    
    ids  = enc["input_ids"].to(DEVICE)
    mask = enc["attention_mask"].to(DEVICE)

    with torch.no_grad():
        ln_norm = model(ids, mask, X_t).cpu().item()
    ln = scaler_y.inverse_transform([[ln_norm]])[0,0]
    T0 = float(row["T"])
    
    return T0 * float(np.exp(ln))
# ================================================

# =============== Telegram notify ==============
def notify(row):
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞
    ticker     = row.get("ticker", "")
    broker_url = f"https://www.tbank.ru/invest/stocks/{ticker}/"
    cur_price  = float(row.get("T") or 0)
    pred       = float(row.get("T+1") or 0)
    pct        = (pred/cur_price - 1) * 100 if cur_price != 0 else 0
    date_str   = row.get("date", "")
    time_str   = row.get("time", "")
    
    # –†–∞–∑–±–∏—Ä–∞–µ–º –≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏ –∏ —Å—Å—ã–ª–∫–∏
    titles = row["news"].split(" || ")
    urls   = row["news_url"].split(" || ")
    # –ü–µ—Ä–µ–≤–æ—Ä–∞—á–∏–≤–∞–µ–º –æ—Ç –Ω–æ–≤–æ–π –∫ —Å—Ç–∞—Ä–æ–π
    all_items   = list(zip(titles, urls))[::-1]
    total_count = len(all_items)

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø—è—Ç—å
    items = all_items[:5]
    shown_count = len(items)

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, —Å–∫–æ–ª—å–∫–æ –∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ –∑–∞–ø—è—Ç–æ–π —É —Ç–µ–∫—É—â–µ–π —Ü–µ–Ω—ã
    raw_T_str = str(cur_price)
    if "." in raw_T_str:
        dec = len(raw_T_str.split(".", 1)[1])
    else:
        dec = 2

    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π
    news_lines = [
        f'‚Ä¢ <a href="{link}">{title}</a>'
        for title, link in items
    ]
    news_block = "\n".join(news_lines)

    stop_loss = cur_price * (1 - pct * 0.5 / 100)
    #relevance = "‚úÖ –°–¥–µ–ª–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞" if abs(pct) > 1 else "‚ùå –°–¥–µ–ª–∫–∞ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞"

    # –°–æ—Å—Ç–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
    direction = "üìà" if pct > 0 else "üìâ"
    lines = [
        f'Smart-Lab',
        f'',
        f'{direction} <a href="{broker_url}"><b>{ticker}</b></a>',
        f'‚è∞ {date_str} {time_str}',
        "",
        f'–¢–µ–∫—É—â–∞—è —Ü–µ–Ω–∞: {cur_price:.{dec}f} —Ä—É–±.',
        f'–ü—Ä–æ–≥–Ω–æ–∑: {pred:.{dec}f} —Ä—É–±. ({pct:+.2f}%)',
        f'–°—Ç–æ–ø-–ª–æ—Å—Å: {stop_loss:.{dec}f} —Ä—É–±.',
        "",
        f'üì∞ <b>–ù–æ–≤–æ—Å—Ç–∏ (–≤—Å–µ–≥–æ {total_count}, –ø–æ–∫–∞–∑–∞–Ω–æ {shown_count})</b>:',
        news_block,
        "",
        #relevance
    ]
    msg = "\n".join(lines)

    # –û—Ç–ø—Ä–∞–≤–∫–∞ —Ñ–æ—Ç–æ + —Å–æ–æ–±—â–µ–Ω–∏—è
    api_url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendPhoto"
    
    # –ü—É—Ç—å –∫ —Ñ–æ—Ç–æ –ø–æ —Ç–∏–∫–µ—Ä—É
    photo_path = f"photo_ticker/{ticker}_photo.jpg"
    if not os.path.exists(photo_path):
        photo_path = "img.jpg"
    
    for cid in CHAT_IDS:
        try:
            # –û—Ç–∫—Ä—ã–≤–∞–µ–º —Ñ–∞–π–ª –ø—Ä—è–º–æ –ø–µ—Ä–µ–¥ –∫–∞–∂–¥—ã–º –∑–∞–ø—Ä–æ—Å–æ–º,
            # —á—Ç–æ–±—ã "photo" –±—ã–ª–æ –≤ –Ω–∞—á–∞–ª–µ —á—Ç–µ–Ω–∏—è
            with open(photo_path, "rb") as photo:
                data = {
                    "chat_id":    cid,
                    "caption":    msg,
                    "parse_mode": "HTML",
                    "disable_web_page_preview": True
                }
                files = {"photo": photo}
                r = requests.post(api_url, data=data, files=files)
                r.raise_for_status()
        except Exception as e:
            print(f"Failed to send Telegram to {cid}: {e}")

# ================================================

# ================ Update + parse ================
def update_csv(ticker, title, url, time_str):
    today = date.today().isoformat()
    rows  = load_rows(ticker)

    def calc_pct(r):
        try:
            t0 = float(r["T"])
            t1 = float(r["T+1"])
            return round((t1/t0 - 1) * 100, 2) if t0 != 0 else None
        except:
            return None

    def maybe_notify(r):
        pct = calc_pct(r)
        if pct is not None and abs(pct) >= 0.5:
            notify(r)

    # 1) –ï—Å–ª–∏ —Å–µ–≥–æ–¥–Ω—è —É–∂–µ –µ—Å—Ç—å —Å—Ç—Ä–æ–∫–∞ ‚Äî –æ–±–Ω–æ–≤–ª—è–µ–º –µ—ë
    for r in rows:
        if r["ticker"] == ticker and r["date"] == today:
            # –æ–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è –∏ —Ü–µ–Ω—ã
            r["time"]       = time_str
            news_dt         = datetime.combine(date.today(),
                                    datetime.strptime(time_str, "%H:%M").time())
            r["T"]          = get_news_price(ticker, news_dt)
            # –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –∏ URL –Ω–æ–≤–æ–π –Ω–æ–≤–æ—Å—Ç–∏
            r["news"]       += " || " + title
            r["news_url"]   += " || " + url
            # –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–æ–≥–Ω–æ–∑
            r["T+1"]        = predict_next_price(r)
            r["pct_change"] = calc_pct(r)

            save_rows(ticker, rows)
            print(f"[{datetime.now()}] {ticker}: –û–ë–ù–û–í–õ–ï–ù–ê –Ω–æ–≤–æ—Å—Ç—å ¬´{title}¬ª, pct={r['pct_change']}%", flush=True)
            maybe_notify(r)
            return

    # 2) –ò–Ω–∞—á–µ ‚Äî —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å
    news_dt = datetime.combine(date.today(),
                datetime.strptime(time_str, "%H:%M").time())
    T0      = get_news_price(ticker, news_dt)
    past    = get_past_prices(load_daily_history(ticker), news_dt.date())

    new_row = {
        "ticker":   ticker,
        "date":     today,
        "time":     time_str,
        "news":     title,
        "news_url": url,
        "T":        T0
    }
    new_row.update(past)
    new_row["T+1"]        = predict_next_price(new_row)
    new_row["pct_change"] = calc_pct(new_row)

    rows.append(new_row)
    save_rows(ticker, rows)
    print(f"[{datetime.now()}] {ticker}: –î–û–ë–ê–í–õ–ï–ù–ê –Ω–æ–≤–æ—Å—Ç—å ¬´{title}¬ª, pct={new_row['pct_change']}%", flush=True)
    maybe_notify(new_row)

def parse_news(ticker):
    """
    –ò—â–µ–º –±–ª–æ–∫ div.temp_block ‚Üí ul.temp_headers--have-numbers ‚Üí –≤—Å–µ <li>,
    —Ñ–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø–æ —Ç–µ–º, –≥–¥–µ –º–µ–∂–¥—É </b> –∏ <a> —Å—Ç–æ–∏—Ç –≤—Ä–µ–º—è HH:MM.
    """
    url  = BASE_URL.format(ticker=ticker)
    resp = safe_get(url)
    if not resp or resp.status_code != 200:
        return []
    soup = BeautifulSoup(resp.text, "html.parser")

    block = soup.find("div", class_="temp_block")
    if not block:
        return []

    ul = block.find("ul", class_="temp_headers temp_headers--have-numbers")
    if not ul:
        return []

    out = []
    for li in ul.find_all("li"):
        b = li.find("b")
        if not b:
            continue
        date_str = None
        for sib in b.next_siblings:
            if getattr(sib, "name", None) == "a":
                break
            if isinstance(sib, str) and sib.strip():
                date_str = sib.strip()
                break
        if not date_str or not re.fullmatch(r"\d{1,2}:\d{2}", date_str):
            continue
        a = li.find("a")
        if not a or not a.get("href"):
            continue
        out.append({
            "title": a.get_text(strip=True),
            "url":   "https://smart-lab.ru" + a["href"],
            "time":  date_str
        })
    return out

def main():
    os.makedirs(DAILY_DATA_DIR, exist_ok=True)
    last_seen = load_last_seen()
    while True:
        for t in TICKERS:
            try:
                news = parse_news(t)
                if not news:
                    continue
                newest = news[0]["url"]
                if t not in last_seen:
                    for it in reversed(news):
                        update_csv(t, it["title"], it["url"], it["time"])
                        for sp in SPECIAL_TICKERS.get(t, []):
                            update_csv(sp, it["title"], it["url"], it["time"])
                    last_seen[t] = newest
                    save_last_seen(last_seen)
                else:
                    to_add = []
                    for it in news:
                        if it["url"] == last_seen[t]:
                            break
                        to_add.append(it)
                    if to_add:
                        for it in reversed(to_add):
                            update_csv(t, it["title"], it["url"], it["time"])
                            for sp in SPECIAL_TICKERS.get(t, []):
                                update_csv(sp, it["title"], it["url"], it["time"])
                        last_seen[t] = newest
                        save_last_seen(last_seen)

            except Exception as e:
                print(f"[ERROR] {t}: {e}", flush=True)

        print(f"[{datetime.now()}] –ó–∞–∫–æ–Ω—á–µ–Ω —Ü–∏–∫–ª –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–ª—è {len(TICKERS)} —Ç–∏–∫–µ—Ä–æ–≤", flush=True)
        time.sleep(CHECK_INTERVAL)

if __name__ == "__main__":
    main()